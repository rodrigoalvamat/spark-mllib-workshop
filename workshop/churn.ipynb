{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab1c41e3ce18de2",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with PySpark MLlib\n",
    "\n",
    "In this lab, we will build a machine learning pipeline using PySpark's MLlib to predict customer churn.\n",
    "\n",
    "We will use the [Telco Customer Churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) dataset from Kaggle, which contains various features about customers that might influence churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676c9adc3bf6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2dff407bf02c48",
   "metadata": {},
   "source": [
    "## Step 1: Initializing Spark Session\n",
    "\n",
    "We start by initializing a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3575a5917fcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"customer-churn-prediction\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ddd49f1d5a04af",
   "metadata": {},
   "source": [
    "## Step 2: Loading the Dataset\n",
    "\n",
    "Next, we load the [Telco Customer Churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) dataset from a CSV file.\n",
    "\n",
    "This dataset includes various features such as customer demographics, account information, and services subscribed to by the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61f7944b468c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "TELCO_DATA = \"../data/telco/customer-churn.csv\"\n",
    "\n",
    "data = spark.read.csv(TELCO_DATA, header=True, inferSchema=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79633d1775f036",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "### 3.1 Handling Non-Numeric Values in `TotalCharges`\n",
    "\n",
    "First, we need to address any non-numeric values in the **TotalCharges** column and convert them to a numerical type.\n",
    "Additionally, we have to remove all rows with **null** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e3b59e93ec0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"TotalCharges\", \n",
    "                       when(regexp_replace(col(\"TotalCharges\"), r\"^[0-9]+(\\.[0-9]+)?$\", col(\"TotalCharges\")).isNull(), None)\n",
    "                       .otherwise(col(\"TotalCharges\")))\n",
    "\n",
    "data = data.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\"))\n",
    "data = data.dropna(subset=[\"TotalCharges\"])\n",
    "\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d08998ccb8257d",
   "metadata": {},
   "source": [
    "### 3.2 Handling Categorical Features\n",
    "\n",
    "We use `StringIndexer` to convert categorical variables into numerical format. This includes features like: \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "  <ul>\n",
    "    <li>gender</li>\n",
    "    <li>Partner</li>\n",
    "    <li>Dependents</li>\n",
    "    <li>PhoneService</li>\n",
    "    <li>MultipleLines</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "  <ul>\n",
    "    <li>InternetService</li>\n",
    "    <li>OnlineSecurity</li>\n",
    "    <li>OnlineBackup</li>\n",
    "    <li>DeviceProtection</li>\n",
    "    <li>TechSupport</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "  <ul>\n",
    "    <li>StreamingTV</li>\n",
    "    <li>StreamingMovies</li>\n",
    "    <li>Contract</li>\n",
    "    <li>PaperlessBilling</li>\n",
    "    <li>PaymentMethod</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "To illustrate the preprocessing, let's take the `gender` feature as an example. \n",
    "\n",
    "| gender | genderIndexed |\n",
    "|--------|---------------|\n",
    "| Female | 0             |\n",
    "| Male   | 1             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c7d07d39960cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(\n",
    "    inputCols=[\n",
    "    'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', \n",
    "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', \n",
    "    'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod'], \n",
    "    outputCols=[\n",
    "    'genderIndexed', 'PartnerIndexed', 'DependentsIndexed', 'PhoneServiceIndexed', 'MultipleLinesIndexed', \n",
    "    'InternetServiceIndexed', 'OnlineSecurityIndexed', 'OnlineBackupIndexed', 'DeviceProtectionIndexed', \n",
    "    'TechSupportIndexed', 'StreamingTVIndexed', 'StreamingMoviesIndexed', 'ContractIndexed', \n",
    "    'PaperlessBillingIndexed', 'PaymentMethodIndexed']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4967633a2821775",
   "metadata": {},
   "source": [
    "### 3.3 Assembling Features\n",
    "\n",
    "We use `VectorAssembler` to combine all feature columns into a single vector column named `features`.\n",
    "\n",
    "For each row, the values from *MonthlyCharges*, *TotalCharges*, *genderIndexed*, and others are concatenated in order to form the feature vector.\n",
    "\n",
    "\n",
    "#### Why Use VectorAssembler?\n",
    "\n",
    "- **Uniform Input Format:** Machine learning algorithms in Spark MLlib require input data to be in the form of a single vector of features. VectorAssembler helps in combining multiple feature columns into this required format.\n",
    "\n",
    "- **Pipeline Compatibility:** It ensures compatibility with the Spark MLlib pipeline, where all feature transformations are applied in sequence before training the model.\n",
    "\n",
    "- **Efficiency:** It consolidates the feature columns into a single vector, which can be processed more efficiently by Spark's distributed computing framework.\n",
    "\n",
    "\n",
    "| id |features             |\n",
    "|----|---------------------|\n",
    "| 0  |[18.0,1.0,0.0,5000.0]|\n",
    "| 1  |[20.0,0.0,1.0,6000.0]|\n",
    "| 2  |[30.0,1.0,0.0,7000.0]|\n",
    "| 3  |[40.0,0.0,1.0,8000.0]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9eba9ad91f2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges', 'genderIndexed', 'PartnerIndexed', \n",
    "        'DependentsIndexed', 'PhoneServiceIndexed', 'MultipleLinesIndexed', 'InternetServiceIndexed', \n",
    "        'OnlineSecurityIndexed', 'OnlineBackupIndexed', 'DeviceProtectionIndexed', 'TechSupportIndexed', \n",
    "        'StreamingTVIndexed', 'StreamingMoviesIndexed', 'ContractIndexed', 'PaperlessBillingIndexed', \n",
    "        'PaymentMethodIndexed'],\n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a615e8103e581",
   "metadata": {},
   "source": [
    "### 3.4 Standardizing Features\n",
    "\n",
    "We use `StandardScaler` to standardize the feature values.\n",
    "\n",
    "Standardization is a preprocessing step in machine learning where features are rescaled so that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4615ce4699bb32",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/normalization-standardization.jpg\" alt=\"Normalization vs Standardization\" width=\"848\" height=\"477\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fd66ce3ef6692",
   "metadata": {},
   "source": [
    "#### Why Standardize?\n",
    "\n",
    "- **Normalization:** Ensures that all features contribute equally to the model, preventing features with larger scales from dominating the learning process.\n",
    "\n",
    "- **Performance:** Many machine learning algorithms (e.g., linear regression, logistic regression, k-means) perform better or converge faster when features are on a similar scale.\n",
    "\n",
    "- **Stability:** Helps in numerical stability, particularly for gradient-based optimization algorithms.\n",
    "\n",
    "#### How Features are Standardized\n",
    "\n",
    "The StandardScaler in PySpark MLlib standardizes features by subtracting the mean and dividing by the standard deviation of each feature. Mathematically, for a feature $x$:\n",
    "\n",
    "$z = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "where:\n",
    "- $x$ is the original feature value.\n",
    "- $\\mu$ is the mean of the feature.\n",
    "- $\\sigma$ is the standard deviation of the feature.\n",
    "- $z$ is the standardized feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6951898568e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a78143c8d3aaa9",
   "metadata": {},
   "source": [
    "### 3.5 Selecting the Target Variable or Label\n",
    "\n",
    "We select the target variable, which is `Churn` in this case.\n",
    "We ensure that only rows with Churn values of Yes or No are kept.\n",
    "Then, we convert the Churn column to a binary format where Yes is mapped to 1.0 and No is mapped to 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51309002a092906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter((col('Churn') == 'Yes') | (col('Churn') == 'No'))\n",
    "data = data.withColumn('label', when(col('Churn') == \"Yes\", 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1748478b354cd77",
   "metadata": {},
   "source": [
    "## Step 4: Splitting the Data\n",
    "\n",
    "We split the dataset into training (80%) and test (20%) sets to allow for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a94b8c6711c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed24d6a2da6150",
   "metadata": {},
   "source": [
    "## Step 5: Initializing the RandomForestClassifier\n",
    "\n",
    "We initialize a random forest classifier, which will be used to predict customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc52caf2149f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(featuresCol='scaledFeatures', labelCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3de514f5737ae7",
   "metadata": {},
   "source": [
    "## Step 6: Creating a Parameter Grid\n",
    "\n",
    "We create a parameter grid for hyperparameter tuning, allowing us to experiment with different values for the number of trees and the maximum depth of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e7d8fe327163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(classifier.maxDepth, [5, 10, 15]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d17bffad90792b",
   "metadata": {},
   "source": [
    "## Step 7: Initializing the Evaluator\n",
    "\n",
    "To evaluate the performance of our binary classification model, we use the ROC curve and the area under the curve (AUC) as our evaluation metrics.\n",
    "\n",
    "\n",
    "- **[ROC Curve](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc):**\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of a classifier's performance across all classification thresholds. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "\n",
    "- **AUC (Area Under the Curve):**\n",
    "The AUC is a single scalar value that summarizes the performance of the classifier. It represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. An AUC value of 1 indicates a perfect model, while an AUC value of 0.5 suggests a model with no discriminative ability.\n",
    "\n",
    "- **BinaryClassificationEvaluator:**\n",
    "In Spark MLlib, the BinaryClassificationEvaluator is used to compute the AUC for binary classification models. It evaluates the model's performance based on the area under the ROC curve (AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d979d2dda2ca3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec79f422a0a1e4a",
   "metadata": {},
   "source": [
    "## Step 8: Initializing the CrossValidator\n",
    "\n",
    "We use cross-validation to perform hyperparameter tuning and model evaluation.\n",
    "\n",
    "K-fold cross validation performs model selection by splitting the dataset into a set of non-overlapping randomly partitioned folds which are used as separate training and test datasets.\n",
    "\n",
    "With k=5 folds, K-fold cross validation will generate 5 (training, test) dataset pairs, each of which uses 4/5 of the data for training and 1/5 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a9eaa2f115d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fb5e69c4674e2",
   "metadata": {},
   "source": [
    "## Step 9: Creating and Fitting the Pipeline\n",
    "\n",
    "We create a pipeline that includes the preprocessing steps, the random forest classifier, and the cross-validator. The model is then fitted to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1d8d6d62c402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer, assembler, scaler, crossval])\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309e34098da9cc5",
   "metadata": {},
   "source": [
    "## Step 10: Making Predictions and Evaluating the Model\n",
    "\n",
    "We make predictions on the test set and evaluate the model using the AUC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cbde547e0a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8239d3aa8be42be",
   "metadata": {},
   "source": [
    "## Step 11: Displaying Feature Importance\n",
    "\n",
    "We extract and plot the feature importances to understand which features are most influential in predicting customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e8a6acad45fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = model.stages[-1].bestModel\n",
    "feature_importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "features = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges', 'genderIndexed', 'PartnerIndexed', \n",
    "            'DependentsIndexed', 'PhoneServiceIndexed', 'MultipleLinesIndexed', 'InternetServiceIndexed', \n",
    "            'OnlineSecurityIndexed', 'OnlineBackupIndexed', 'DeviceProtectionIndexed', 'TechSupportIndexed', \n",
    "            'StreamingTVIndexed', 'StreamingMoviesIndexed', 'ContractIndexed', 'PaperlessBillingIndexed', \n",
    "            'PaymentMethodIndexed']\n",
    "\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_df)\n",
    "plt.xlabel('Feature Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.title('Feature Importance for Customer Churn Prediction', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b581307d94ad786",
   "metadata": {},
   "source": [
    "## Step 12: Closing the Spark Session\n",
    "\n",
    "Finally, we close the Spark session to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcb07737aad001",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

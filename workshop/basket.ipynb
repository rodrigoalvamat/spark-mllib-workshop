{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd0a1945d628e62",
   "metadata": {},
   "source": [
    "# Market Basket Analysis using Apache Spark\n",
    "\n",
    "In this notebook, we will perform a Market Basket Analysis using the Instacart dataset.\n",
    "The dataset contains over 3 million grocery orders from more than 200,000 users.\n",
    "We will use Apache Spark and its MLlib library to perform frequent pattern mining and association rule mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781c8ca",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/basket-equation.png\" alt=\"Support, Confidence, and Lift metrics\" width=\"882\" height=\"446\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b358f",
   "metadata": {},
   "source": [
    "## Understanding Support, Confidence, and Lift\n",
    "\n",
    "In Market Basket Analysis, we use three main metrics to evaluate association rules: Support, Confidence, and Lift.\n",
    "\n",
    "- **Support** \n",
    "\n",
    "This measures how frequently the itemset appears in the dataset. It is calculated as the proportion of transactions that contain the itemset.\n",
    "\n",
    "$\\text{Support}(X \\Rightarrow Y) = \\frac{\\text{freq}(X, Y)}{N}$\n",
    "\n",
    "Where $\\text{freq}(X, Y)$ is the number of transactions containing both $X$ and $Y$, and $N$ is the total number of transactions.\n",
    "\n",
    "- **Confidence**\n",
    "\n",
    "This measures how often items in $Y$ appear in transactions that contain $X$.\n",
    "It is calculated as the proportion of transactions containing $X$ that also contain $Y$.\n",
    "\n",
    "$\\text{Confidence}(X \\Rightarrow Y) = \\frac{\\text{freq}(X, Y)}{\\text{freq}(X)}$\n",
    "\n",
    "Where $\\text{freq}(X)$ is the number of transactions containing $X$.\n",
    "\n",
    "- **Lift**\n",
    "\n",
    "This measures the strength of an association rule over the random co-occurrence of $X$ and $Y$, providing an indicator of the importance of the rule.\n",
    "\n",
    "$\\text{Lift}(X \\Rightarrow Y) = \\frac{\\text{Support}(X \\Rightarrow Y)}{\\text{Supp}(X) \\times \\text{Supp}(Y)}$\n",
    "  \n",
    "Where $\\text{Supp}(X)$ and $\\text{Supp}(Y)$ are the individual supports of $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab203700",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "First, we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "id": "ee4395e1a8f3830f",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_set, expr\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from wordcloud import WordCloud"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bca8645e",
   "metadata": {},
   "source": [
    "## Initializing Spark Session\n",
    "\n",
    "We will now initialize a Spark session with the necessary configurations."
   ]
  },
  {
   "cell_type": "code",
   "id": "9d307028bda2bedb",
   "metadata": {},
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"basket-analysis\")\n",
    "         .enableHiveSupport()\n",
    "         .config(\"spark.driver.memory\", \"3g\")\n",
    "         .config(\"spark.executor.memory\", \"3g\")\n",
    "         .getOrCreate())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "095299b3",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "Next, we will load the Instacart dataset into Spark DataFrames.\n",
    "This dataset includes several CSV files, each containing different aspects of the order data.\n",
    "\n",
    "The dataset can be found on [Kaggle](https://www.kaggle.com/c/instacart-market-basket-analysis/data).\n",
    "\n",
    "Due to GitHub's file size limits, the dataset is not included directly in the repository.\n",
    "Please download the dataset from Kaggle, extract the files, and place them in the `data` folder according to the paths specified in the Spark read operations."
   ]
  },
  {
   "cell_type": "code",
   "id": "72d12734",
   "metadata": {},
   "source": [
    "INSTACART_DATA = \"../data/instacart\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "aisles = spark.read.csv(f\"{INSTACART_DATA}/aisles.csv\", header=True, inferSchema=True)\n",
    "departments = spark.read.csv(f\"{INSTACART_DATA}/departments.csv\", header=True, inferSchema=True)\n",
    "order_products_prior = spark.read.csv(f\"{INSTACART_DATA}/order_products__prior.csv\", header=True, inferSchema=True)\n",
    "order_products_train = spark.read.csv(f\"{INSTACART_DATA}/order_products__train.csv\", header=True, inferSchema=True)\n",
    "orders = spark.read.csv(f\"{INSTACART_DATA}/orders.csv\", header=True, inferSchema=True)\n",
    "products = spark.read.csv(f\"{INSTACART_DATA}/products.csv\", header=True, inferSchema=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "79876da0",
   "metadata": {},
   "source": [
    "## Creating Temporary Views\n",
    "\n",
    "We will create temporary views for each of the DataFrames to enable SQL querying."
   ]
  },
  {
   "cell_type": "code",
   "id": "1ec3c91e4ebe9368",
   "metadata": {},
   "source": [
    "aisles.createOrReplaceTempView(\"aisles\")\n",
    "departments.createOrReplaceTempView(\"departments\")\n",
    "order_products_prior.createOrReplaceTempView(\"order_products_prior\")\n",
    "order_products_train.createOrReplaceTempView(\"order_products_train\")\n",
    "orders.createOrReplaceTempView(\"orders\")\n",
    "products.createOrReplaceTempView(\"products\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f53ae2a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### Viewing the First Few Rows of Each Table\n",
    "\n",
    "Let's take a look at the first few rows of each of the imported files to understand their structure and contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed9788766619b47f",
   "metadata": {},
   "source": [
    "orders.show(n=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4523bf707c157f9e",
   "metadata": {},
   "source": [
    "products.show(n=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "362ae9796e755256",
   "metadata": {},
   "source": [
    "aisles.show(n=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cadc4e977c8f1ed",
   "metadata": {},
   "source": [
    "departments.show(n=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4baea97db4c323e",
   "metadata": {},
   "source": [
    "order_products_train.show(n=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49a3be1e79d39cf5",
   "metadata": {},
   "source": [
    "order_products_prior.show(n=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03b45282",
   "metadata": {},
   "source": [
    "### Orders by Hour of the Day\n",
    "\n",
    "We will now analyze the distribution of orders by the hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "id": "ff6c904285252110",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(order_id) AS total_orders, order_hour_of_day AS hour \n",
    "FROM orders \n",
    "GROUP BY order_hour_of_day \n",
    "ORDER BY order_hour_of_day\n",
    "\"\"\"\n",
    "orders_by_hour = spark.sql(query)\n",
    "orders_by_hour.show(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "babdd8fa",
   "metadata": {},
   "source": [
    "### Visualizing Orders by Hour\n",
    "\n",
    "Let's visualize the number of orders placed at each hour of the day using a line plot."
   ]
  },
  {
   "cell_type": "code",
   "id": "9f382cf2abbeb4f2",
   "metadata": {},
   "source": [
    "orders_by_hour_plot = orders_by_hour.toPandas()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='hour', y='total_orders', data=orders_by_hour_plot, marker='o', markersize=6)\n",
    "\n",
    "for i in range(orders_by_hour_plot.shape[0]):\n",
    "    plt.scatter(orders_by_hour_plot['hour'][i], orders_by_hour_plot['total_orders'][i], s=200, facecolors='none', edgecolors='r')\n",
    "\n",
    "plt.title('Total Orders by Hour of the Day')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Total Orders')\n",
    "plt.xticks(range(24))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d885b25",
   "metadata": {},
   "source": [
    "### Orders by Days Since Prior Order\n",
    "\n",
    "Next, we will analyze how often customers place orders by looking at the days since their prior order."
   ]
  },
  {
   "cell_type": "code",
   "id": "d2635caf4a1cb8ea",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT days_since_prior_order, COUNT(order_id) AS total_orders\n",
    "FROM orders \n",
    "GROUP BY days_since_prior_order \n",
    "ORDER BY days_since_prior_order\n",
    "\"\"\"\n",
    "days_since_prior_order = spark.sql(query)\n",
    "days_since_prior_order.show(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a02bd6f7",
   "metadata": {},
   "source": [
    "### Visualizing Orders by Days Since Prior Order\n",
    "\n",
    "We will visualize the distribution of orders based on the days since the prior order using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "id": "d9dd114465d1e51",
   "metadata": {},
   "source": [
    "days_since_prior_order_plot = days_since_prior_order.toPandas()\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='days_since_prior_order', y='total_orders', data=days_since_prior_order_plot, \n",
    "            palette='viridis', hue='days_since_prior_order', dodge=False, legend=False)\n",
    "\n",
    "plt.title('Total Orders by Days Since Prior Order')\n",
    "plt.xlabel('Days Since Prior Order')\n",
    "plt.ylabel('Total Orders')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a74fc68c",
   "metadata": {},
   "source": [
    "### Orders by Day of the Week\n",
    "\n",
    "Now, let's analyze on which day of the week customers make the most purchases."
   ]
  },
  {
   "cell_type": "code",
   "id": "d36c41a16c509826",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(order_id) AS total_orders, \n",
    "  (CASE \n",
    "     WHEN order_dow = '0' THEN 'Sunday'\n",
    "     WHEN order_dow = '1' THEN 'Monday'\n",
    "     WHEN order_dow = '2' THEN 'Tuesday'\n",
    "     WHEN order_dow = '3' THEN 'Wednesday'\n",
    "     WHEN order_dow = '4' THEN 'Thursday'\n",
    "     WHEN order_dow = '5' THEN 'Friday'\n",
    "     WHEN order_dow = '6' THEN 'Saturday'              \n",
    "   end) as day_of_week \n",
    "  FROM orders  \n",
    " GROUP BY order_dow \n",
    " ORDER BY total_orders desc\n",
    "\"\"\"\n",
    "order_by_weekday = spark.sql(query)\n",
    "order_by_weekday.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be670966",
   "metadata": {},
   "source": [
    "### Visualizing Orders by Day of the Week\n",
    "\n",
    "We will visualize the number of orders placed on each day of the week using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "id": "7bec1c4f06ee9a1",
   "metadata": {},
   "source": [
    "order_by_weekday_plot = order_by_weekday.toPandas()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='day_of_week', y='total_orders', hue='day_of_week', data=order_by_weekday_plot,\n",
    "            palette='viridis', dodge=False, legend=False)\n",
    "\n",
    "plt.title('Total Orders by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Orders')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81579fc2",
   "metadata": {},
   "source": [
    "### Creating a Master Table\n",
    "\n",
    "Let's create a master table by merging the product, department, order_products_train, and order_products_prior datasets.\n",
    "This will help us in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "11a43d1acc1748cd",
   "metadata": {},
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS order_items_temp\")\n",
    "\n",
    "query = \"\"\"\n",
    "CREATE TABLE order_items_temp AS\n",
    "(\n",
    "    SELECT order_products.*, products.product_name, products.aisle_id, products.department_id, departments.department\n",
    "    FROM\n",
    "    (\n",
    "        SELECT * FROM order_products_train \n",
    "        UNION\n",
    "        SELECT * FROM order_products_prior\n",
    "    ) AS order_products\n",
    "    INNER JOIN products\n",
    "    ON order_products.product_id = products.product_id\n",
    "    INNER JOIN departments\n",
    "    ON products.department_id = departments.department_id\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80482711",
   "metadata": {},
   "source": [
    "### Number of Items per Order\n",
    "\n",
    "Now, let's find out how many items customers typically purchase in a single order."
   ]
  },
  {
   "cell_type": "code",
   "id": "5f6e95fdf1c813ff",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT order_id, COUNT(product_id) AS total_items\n",
    "FROM order_items_temp \n",
    "GROUP BY order_id\n",
    "\"\"\"\n",
    "items_by_order = spark.sql(query)\n",
    "items_by_order.show(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8cd0f550ff696774",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT total_items, COUNT(order_id) AS num_orders\n",
    "FROM\n",
    "(\n",
    "    SELECT order_id, COUNT(product_id) AS total_items\n",
    "    FROM order_items_temp \n",
    "    GROUP BY order_id\n",
    ") AS items_by_order\n",
    "GROUP BY total_items\n",
    "ORDER BY total_items\n",
    "\"\"\"\n",
    "items_by_order_aggregated = spark.sql(query)\n",
    "items_by_order_aggregated.show(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef8b1c71",
   "metadata": {},
   "source": [
    "### Visualizing Number of Items per Order\n",
    "\n",
    "We will visualize the distribution of the number of items per order using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "id": "1330d777a7344659",
   "metadata": {},
   "source": [
    "items_by_order_plot = items_by_order_aggregated.toPandas()\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "bar_plot = sns.barplot(x='total_items', y='num_orders', data=items_by_order_plot, \n",
    "            hue='total_items', palette='viridis', dodge=False, legend=False)\n",
    "\n",
    "bar_plot.xaxis.set_major_locator(ticker.MultipleLocator(4))\n",
    "bar_plot.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x/1000)}k'))\n",
    "\n",
    "plt.title('Number of Orders by Total Items')\n",
    "plt.xlabel('Total Items')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cb6af8ef",
   "metadata": {},
   "source": [
    "### Orders by Department\n",
    "\n",
    "Next, we will analyze which departments have the most orders."
   ]
  },
  {
   "cell_type": "code",
   "id": "dcd5a53f9168316a",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT department, COUNT(*) AS orders_count from order_items_temp\n",
    "GROUP BY department\n",
    "ORDER BY orders_count desc\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "orders_by_department = spark.sql(query)\n",
    "orders_by_department.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed03d448",
   "metadata": {},
   "source": [
    "### Visualizing Orders by Department\n",
    "\n",
    "We will visualize the top 10 departments based on the number of orders using a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "id": "83c4618aa1959766",
   "metadata": {},
   "source": [
    "orders_by_department_plot = orders_by_department.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.pie(orders_by_department_plot['orders_count'], labels=orders_by_department_plot['department'],\n",
    "        autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "plt.legend(orders_by_department_plot['department'], title=\"Departments\", bbox_to_anchor=(1.05, 1), loc='best')\n",
    "\n",
    "plt.title('Top 10 Departments by Order Count')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "053a44d0",
   "metadata": {},
   "source": [
    "### Most Purchased Products\n",
    "\n",
    "Let's find out which products are the most purchased by customers."
   ]
  },
  {
   "cell_type": "code",
   "id": "af23031ab1a7c873",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT product_name, COUNT(*) AS orders_count from order_items_temp\n",
    "GROUP BY product_name\n",
    "ORDER BY orders_count desc\n",
    "LIMIT 200\n",
    "\"\"\"\n",
    "product_by_order = spark.sql(query)\n",
    "product_by_order.show(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edb270c7a137b1ad",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT product_name\n",
    "FROM (\n",
    "  SELECT product_name, count(*) AS orders_count\n",
    "  FROM order_items_temp\n",
    "  GROUP BY product_name\n",
    "  ORDER BY orders_count DESC\n",
    "  LIMIT 200\n",
    ")\n",
    "\"\"\"\n",
    "words_df = spark.sql(query)\n",
    "words = words_df.rdd.flatMap(lambda x: x).collect()\n",
    "words_str = ' '.join(words)\n",
    "word_cloud = WordCloud(background_color=\"white\").generate(words_str)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04a6e5e0",
   "metadata": {},
   "source": [
    "### Visualizing Most Purchased Products\n",
    "\n",
    "We will create a word cloud to visualize words related to the most frequently purchased products."
   ]
  },
  {
   "cell_type": "code",
   "id": "795d78a85bbd8a37",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "display()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d5b8e6d",
   "metadata": {},
   "source": [
    "## FP-Growth Algorithm\n",
    "\n",
    "The FP-Growth algorithm is used for frequent pattern mining.\n",
    "Let's implement this algorithm to find frequent itemsets and association rules in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cad996",
   "metadata": {},
   "source": [
    "### Organizing Data by Shopping Basket\n",
    "\n",
    "First, we need to organize our data into shopping baskets.\n",
    "We will create a DataFrame where each row represents a single order and contains a list of items in that order."
   ]
  },
  {
   "cell_type": "code",
   "id": "b111f1355d3d7e05",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT products.product_name, order_products.order_id \n",
    "FROM products \n",
    "INNER JOIN order_products_train AS order_products  \n",
    "WHERE order_products.product_id = products.product_id\n",
    "\"\"\"\n",
    "\n",
    "raw_data = spark.sql(query)\n",
    "raw_data.show(5, truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61e60f8044b7506",
   "metadata": {},
   "source": [
    "baskets = raw_data.groupBy('order_id').agg(collect_set('product_name').alias('items'))\n",
    "baskets.createOrReplaceTempView('baskets')\n",
    "baskets.show(5, truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "847a34afa5a51e27",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT items from baskets\n",
    "\"\"\" \n",
    "\n",
    "baskets_items = spark.sql(query).withColumn('items', expr('TRANSFORM(items, x -> CAST(x AS STRING))'))\n",
    "baskets_items.show(5, truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b032495d",
   "metadata": {},
   "source": [
    "### Implementing FP-Growth Algorithm\n",
    "\n",
    "Next, we will use the FP-Growth algorithm from PySpark's MLlib to find frequent itemsets and generate association rules."
   ]
  },
  {
   "cell_type": "code",
   "id": "b662962dae42b075",
   "metadata": {},
   "source": [
    "fpgrowth = FPGrowth().setItemsCol(\"items\").setMinSupport(0.001).setMinConfidence(0)\n",
    "model = fpgrowth.fit(baskets_items)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98c696e9",
   "metadata": {},
   "source": [
    "### Frequent Itemsets\n",
    "\n",
    "Let's take a look at the most frequent itemsets found by the FP-Growth algorithm."
   ]
  },
  {
   "cell_type": "code",
   "id": "5867a5f1ccf08a55",
   "metadata": {},
   "source": [
    "most_popular_item_in_basket = model.freqItemsets\n",
    "most_popular_item_in_basket.createOrReplaceTempView(\"most_popular_item_in_basket\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c440ca0ca019f2f",
   "metadata": {},
   "source": [
    "if_then = model.associationRules\n",
    "if_then.createOrReplaceTempView(\"if_then\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "965bc06a8a33499b",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT items, freq\n",
    "FROM most_popular_item_in_basket \n",
    "WHERE SIZE(items) > 2 \n",
    "ORDER BY freq desc\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "items_freq = spark.sql(query)\n",
    "items_freq.show(5, truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78e74d39",
   "metadata": {},
   "source": [
    "### Association Rules\n",
    "\n",
    "We will now examine the association rules generated by the FP-Growth algorithm, focusing on those with high confidence and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed0849",
   "metadata": {},
   "source": [
    "### High Confidence Rules\n",
    "\n",
    "Here are the association rules with the highest confidence values."
   ]
  },
  {
   "cell_type": "code",
   "id": "24bf4a5174c01192",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT antecedent AS `antecedent (if)`, consequent AS `consequent (then)`, confidence \n",
    "FROM if_then \n",
    "ORDER BY confidence DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "confidence = spark.sql(query)\n",
    "confidence.show(5, truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d07a3dd",
   "metadata": {},
   "source": [
    "### High Lift Rules\n",
    "\n",
    "Finally, let's look at the association rules with the highest lift values."
   ]
  },
  {
   "cell_type": "code",
   "id": "55d630ce175bc930",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM if_then \n",
    "WHERE lift > 1\n",
    "ORDER BY lift DESC\n",
    "\"\"\"\n",
    "\n",
    "lift = spark.sql(query)\n",
    "lift.show(5, truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5965a88b",
   "metadata": {},
   "source": [
    "## Step 12: Closing the Spark Session\n",
    "\n",
    "Finally, we close the Spark session to release resources."
   ]
  },
  {
   "cell_type": "code",
   "id": "32388458",
   "metadata": {},
   "source": [
    "spark.stop()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
